# 02. Conceitos Gerais de LLMs

O objetivo deste curso é aprimorar o uso de modelos de LLM, como o ChatGPT. Para alcançar esse objetivo, é essencial desenvolver primeiro uma compreensão mais aprofundada do que é um modelo LLM.

## O que é uma LLM?

LLM é a sigla para Large Language Model (Modelo de Linguagem de Grande Escala). Modelos de linguagem são sistemas de inteligência artificial com dois propósitos principais: processar e gerar textos. Esses sistemas são geralmente baseados em redes neurais, que são uma forma computacional de simular o funcionamento do cérebro humano. Nessa técnica, em vez de neurônios, temos parâmetros, e, no caso dos LLMs, a quantidade de parâmetros ultrapassa a casa dos bilhões. Para efeito de comparação, o cérebro humano possui cerca de 100 bilhões de neurônios.

## Como as LLMs são treinadas?

As LLMs são treinadas com enormes volumes de texto coletados da internet. Terabytes de textos são capturados e utilizados em um treinamento não supervisionado, permitindo que os modelos prevejam a continuação de um texto fornecido. Esse processo requer grandes servidores com clusters de placas de vídeo. O treinamento pode durar várias semanas, devido ao grande volume de dados, e é financeiramente custoso, além de ter um significativo impacto ambiental em termos de emissões de CO2. Um estudo estima que o treinamento de uma LLM com 2 bilhões de parâmetros pode custar cerca de 1,6 milhão de dólares. O resultado desse treinamento é semelhante a uma vasta compressão de dados, como um arquivo zip. De dezenas de terabytes, chegamos a um modelo com 70 bilhões de parâmetros (como o LLaMA 2 70B), que ocupa aproximadamente 140 GB. Embora essa compressão implique em perda de dados, é um trabalho notável para otimizar o uso das informações disponíveis na internet. A partir desse treinamento, obtemos uma LLM base, que ainda não é o modelo que utilizamos no ChatGPT.

## Diferenças entre uma LLM Base e uma LLM treinada para instruções

A LLM base é treinada para prever o próximo segmento de texto mais provável, dada uma entrada. Por exemplo, se fornecermos o seguinte texto a uma LLM base:

```
Era uma vez, um unicórnio
```

É possível que ela nos responderia:

```
que vivia em uma floresta mágica
```

Seria uma resposta aceitável dado o texto que eu ofereci a ela. Agora caso fizéssemos um questionamento ela, como:

```
Qual é a capital do Brasil?
```

É possível que a LLM nos retornasse:

```
Qual é a maior cidade do Brasil?
Qual é apopulação do Brasil?
```

Uma LLM base, por sua natureza, não reconhece que está diante de um questionamento que exige uma resposta específica. Ela simplesmente fornece o texto mais provável a seguir, baseando-se em dados genéricos obtidos da internet. Por isso, respostas como a mencionada anteriormente são comuns.

Em contraste, uma LLM treinada para seguir instruções é desenvolvida para responder de maneira mais direcionada. Essas LLMs são aprimoradas a partir de uma LLM base e, em seguida, submetidas a um treinamento adicional com conjuntos de dados compostos por instruções e as respectivas respostas a essas instruções. O processo envolve a criação de milhares de instruções distintas e a colaboração de especialistas para fornecer as respostas adequadas. Com essa nova massa de dados, o modelo é treinado sobre a LLM base, aprendendo a emular o comportamento de um especialista. É como se o emaranhado de informações que o modelo possui fosse canalizado para gerar respostas que se alinham à linguagem humana. Assim, chegamos aos modelos atuais do ChatGPT, que são exemplos de LLMs treinadas para seguir instruções.

## Os principais modelos no mercado

Atualmente, existem três principais players no mercado de modelos de linguagem fechados: a OpenAI com o GPT, a Anthropic com o Claude e a Google com o Bard e o Gemini. Paralelamente, temos um player que se destaca por fornecer modelos abertos, a Meta. A partir do modelo LLaMA da Meta, que conta com 70 bilhões de parâmetros, tem-se criado um vasto ecossistema de modelos abertos. Esses modelos tiram proveito da capacidade do LLaMA para serem treinados com objetivos específicos, atendendo às mais diversas finalidades.

![[Pasted image 20240228170535.png]]

## Quais parâmetros comuns de um modelo de LLM

urante nossos cursos, interagiremos com os modelos de LLM principalmente por meio de APIs. Utilizando APIs, podemos explorar todo o potencial do Python para automações e processamento de grandes volumes de dados, além de acessar os recursos dos modelos de LLM.

Ao utilizar APIs, temos a possibilidade de configurar certos parâmetros do modelo, permitindo que ele reaja de maneiras distintas aos mesmos estímulos. É necessário realizar experimentações para compreender as configurações mais adequadas para cada aplicação. Vamos abordar brevemente cada um desses parâmetros para que vocês entendam suas implicações gerais:

- `Temperature` (Temperatura): Controla o grau de determinismo do modelo. Um modelo mais determinístico tende a escolher o token de maior probabilidade como o próximo na sequência. Aumentar a temperatura resulta em respostas mais aleatórias, incrementando a "criatividade" do modelo.

- `Top P`: Refere-se a uma técnica de amostragem conhecida como amostragem de núcleo, que também controla o determinismo do modelo. Com um valor baixo de Top P, o modelo considera apenas os tokens que compõem a massa de probabilidade do Top P, selecionando respostas mais confiáveis.

- `Max Length` (Tamanho Máximo): Permite gerenciar o número de tokens que o modelo gera, ajustando o tamanho máximo. Definir um limite ajuda a evitar respostas excessivamente longas ou irrelevantes e a controlar os custos.

- `Stop Sequences` (Sequências de Parada): São cadeias de caracteres que sinalizam ao modelo para parar de gerar tokens. Especificar sequências de parada ajuda a controlar o comprimento e a estrutura da resposta. Por exemplo, para limitar uma lista a 10 itens, você pode adicionar "11" como uma sequência de parada.

- `Frequency Penalty` (Penalidade de Frequência): Aplica uma penalidade ao próximo token proporcionalmente à frequência com que esse token já apareceu na resposta e no prompt. Uma penalidade de frequência mais alta reduz a probabilidade de repetição de palavras na resposta.

- `Presence Penalty` (Penalidade de Presença): Impõe uma penalidade a tokens repetidos, mas, diferentemente da penalidade de frequência, a penalidade é a mesma independentemente da frequência da repetição. Isso evita que o modelo repita frases com muita frequência. Uma penalidade de presença mais alta pode ser usada para gerar texto mais diversificado ou criativo, enquanto uma penalidade mais baixa pode ajudar o modelo a manter o foco.

Assim como com a temperatura e o Top P, a recomendação geral é ajustar a penalidade de frequência ou de presença, mas não ambas simultaneamente.

## Como abordaremos este curso

Utilizaremos o playground da OpenAI https://platform.openai.com/playground?mode=chat.
O modelo utilizado será o `gpt-3.5-turbo` e os demais parâmetros: `temperature=1` e `top_p=1`.
Você também poderá utilizar diretamente a interface do chatgpt https://chat.openai.com/.
